# 🥷 ai-jailbreak-jutsu

💥 A curated collection of proof-of-concept (PoC) attacks against Large Language Models (LLMs), including prompt injection, model extraction, data leakage, and more.

## 🔍 Contents
- 🧠 Prompt Injection Techniques
- 📦 Model Extraction PoC
- 📤 Data Leakage Experiments
- 🔒 Defense Evasion (LLMGuard, Rebuff, etc.)
- ⚙️ Automated Prompt Chains (WIP)

## 🧪 Why this repo?
This project aims to:
- Serve as a **public testing ground** for LLM red-teaming
- Provide **reproducible PoCs** with tools like OpenAI, Ollama, Claude, etc.
- Promote **responsible disclosure** and awareness

## ⚠️ Disclaimer
This repository is intended for **educational and ethical purposes only**.  
**Do not use it on systems you don’t own or have permission to test.**

## 📄 License
Apache License 2.0 – See [LICENSE](./LICENSE)
