# ğŸ¥· ai-jailbreak-jutsu

ğŸ’¥ A curated collection of proof-of-concept (PoC) attacks against Large Language Models (LLMs), including prompt injection, model extraction, data leakage, and more.

## ğŸ” Contents
- ğŸ§  Prompt Injection Techniques
- ğŸ“¦ Model Extraction PoC
- ğŸ“¤ Data Leakage Experiments
- ğŸ”’ Defense Evasion (LLMGuard, Rebuff, etc.)
- âš™ï¸ Automated Prompt Chains (WIP)

## ğŸ§ª Why this repo?
This project aims to:
- Serve as a **public testing ground** for LLM red-teaming
- Provide **reproducible PoCs** with tools like OpenAI, Ollama, Claude, etc.
- Promote **responsible disclosure** and awareness

## âš ï¸ Disclaimer
This repository is intended for **educational and ethical purposes only**.  
**Do not use it on systems you donâ€™t own or have permission to test.**

## ğŸ“„ License
Apache License 2.0 â€“ See [LICENSE](./LICENSE)
